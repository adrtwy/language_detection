{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to create a language detection model using NLTK and the [European Parliament Proceedings Parallel Corpus](https://statmt.org/europarl/).\n",
    "\n",
    "General Outline:\n",
    "1. Download the [europarl corpus](https://statmt.org/europarl/v7/europarl.tgz)\n",
    "2. Preprocess the data\n",
    "3. Train a model\n",
    "4. Test the model\n",
    "5. Create a function that allows user input and returns the language of the sample text\n",
    "6. Next steps and Conclusions\n",
    "\n",
    "The code will be entirely in Python.\n",
    "\n",
    "\n",
    "Limitations:\n",
    "Due to the file size restriction on GitHub & my computer, this repo and model will only be built off of very small samples of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_url = r'https://statmt.org/europarl/v7/europarl.tgz'\n",
    "\n",
    "#r = requests.get(corpus_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the corpus\n",
    "# I downloaded the file directly to speed up the process\n",
    "file = tarfile.open(r'europarl.tgz')\n",
    "print(file.getnames())\n",
    "\n",
    "file.extractall('./corpus_folder')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guide to the language labels:\n",
    "\n",
    "Label | Language\n",
    "|----|----|\n",
    "bg | Bulgarian\n",
    "cs| Czech\n",
    "da| Danish\n",
    "de|German\n",
    "el|Greek\n",
    "en|English\n",
    "es|Spanish\n",
    "et|Estonian\n",
    "fi|Finnish\n",
    "fr|French\n",
    "hu|Hungarian\n",
    "it|Italian\n",
    "lt|Lithuanian\n",
    "lv|Latvian\n",
    "nl|Dutch\n",
    "pl|Polish\n",
    "pt|Portuguese\n",
    "ro|Romanian\n",
    "sk|Slovak\n",
    "sl|Slovene\n",
    "sv|Swedish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CHAPTER ID=\"011\">\n",
      "Въпроси с искане за устен отговор и писмени декларации (внасяне): вж. протокола\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of one of the txt files within the corpus\n",
    "# Make sure to change the encoding\n",
    "ex_txt = open(r\".\\corpus_folder\\txt\\bg\\ep-07-01-15-011.txt\",\"r\", encoding=\"utf8\")\n",
    "\n",
    "print(ex_txt.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to turn these into lines in a csv file\n",
    "# That means I'll need to combine all the files into a single txt file\n",
    "# I then need to remove the <CHAPTER ID> headings\n",
    "# Then I'll want to remove the punctuation before inputting everything into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new compiled folder outside of the txt folder\n",
    "# This is where the results will be sent\n",
    "directory = \"compiled\"\n",
    "    \n",
    "# Parent Directory path \n",
    "parent_dir = r\".\\corpus_folder\"\n",
    "    \n",
    "# Path \n",
    "path = os.path.join(parent_dir, directory) \n",
    "    \n",
    "# Create the directory\n",
    "os.mkdir(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign directory\n",
    "directory = r'corpus_folder\\txt'\n",
    "\n",
    "\n",
    "# iterate over all folders in that directory\n",
    "for folder in os.scandir(directory):\n",
    "    if folder.is_dir():\n",
    "        lang = folder.path[-2:]\n",
    "\n",
    "    \n",
    "    # iterate over files in that directory\n",
    "    for filename in os.scandir(folder):\n",
    "        if filename.is_file():\n",
    "            print(filename.path)\n",
    "\n",
    "        #Open the file, read the file\n",
    "        file1 = open(filename.path,\"r\", encoding=\"utf8\")\n",
    "        excerpt = file1.readlines()\n",
    "\n",
    "        #Open file2, append to file2\n",
    "        output_path = r'corpus_folder/compiled/' + lang + '_comp.txt' \n",
    "        #os.mkdir(output_path)\n",
    "\n",
    "        file2 = open(output_path,\"a\", encoding=\"utf8\")\n",
    "        file2.writelines(line + '\\n' for line in excerpt) #This adds each new line to a new line in the output file\n",
    "\n",
    "        file1.close()\n",
    "        file2.close()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was an error with \"\\pl\\ep-09-10-22-010.txt\"\n",
    "# I'll just skip the rest of the polish\n",
    "# This is working now, so I'm not sure what the issue was"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have each folder's worth of information in a single sheet,\n",
    "I will turn each sheet into a csv, and then into a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to remove all html & punctuation, to convert to a csv\n",
    "# Then output to a separate folder\n",
    "\n",
    "directory = \"cleaned\"\n",
    "# Parent Directory path \n",
    "parent_dir = r\".\\corpus_folder\"\n",
    "    \n",
    "# Path \n",
    "path = os.path.join(parent_dir, directory) \n",
    "    \n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the compiled txt file, then removes all html, and punctuation\n",
    "def clean_txt (file):\n",
    "    in_file = open(file, encoding='utf8').read()\n",
    "\n",
    "    lang = os.path.basename(file)[:2]\n",
    "    out_file = './corpus_folder/cleaned/' +lang +'_clean.txt'\n",
    "    clean_ex = re.sub(r'<.*?>', '', in_file).lower().strip()\n",
    "    #print(out_file)\n",
    "\n",
    "    # Here i'll also remove commas, which will make the conversion to csv simpler\n",
    "    # This will not affect the process because punctuation will be completely \n",
    "    # removed during the normalization process\n",
    "    #clean_ex = re.sub(',', '', clean_ex)\n",
    " \n",
    "    open(out_file, 'w', encoding='utf8').write(clean_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_folder\\compiled\\bg_comp.txt\n",
      "corpus_folder\\compiled\\cs_comp.txt\n",
      "corpus_folder\\compiled\\da_comp.txt\n",
      "corpus_folder\\compiled\\de_comp.txt\n",
      "corpus_folder\\compiled\\el_comp.txt\n",
      "corpus_folder\\compiled\\en_comp.txt\n",
      "corpus_folder\\compiled\\es_comp.txt\n",
      "corpus_folder\\compiled\\et_comp.txt\n",
      "corpus_folder\\compiled\\fi_comp.txt\n",
      "corpus_folder\\compiled\\fr_comp.txt\n",
      "corpus_folder\\compiled\\hu_comp.txt\n",
      "corpus_folder\\compiled\\it_comp.txt\n",
      "corpus_folder\\compiled\\lt_comp.txt\n",
      "corpus_folder\\compiled\\lv_comp.txt\n",
      "corpus_folder\\compiled\\nl_comp.txt\n",
      "corpus_folder\\compiled\\pl_comp.txt\n",
      "corpus_folder\\compiled\\pt_comp.txt\n",
      "corpus_folder\\compiled\\ro_comp.txt\n",
      "corpus_folder\\compiled\\sk_comp.txt\n",
      "corpus_folder\\compiled\\sl_comp.txt\n",
      "corpus_folder\\compiled\\sv_comp.txt\n"
     ]
    }
   ],
   "source": [
    "folder = r'corpus_folder\\compiled'\n",
    "\n",
    "for filename in os.scandir(folder):\n",
    "    clean_txt(filename)\n",
    "    print(filename.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"csv\"\n",
    "# Parent Directory path \n",
    "parent_dir = r\".\\corpus_folder\"\n",
    "    \n",
    "# Path \n",
    "path = os.path.join(parent_dir, directory) \n",
    "    \n",
    "# Create the directory\n",
    "#os.mkdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'corpus_folder\\cleaned'\n",
    "\n",
    "# Take a random sample of each\n",
    "for filename in os.scandir(folder):\n",
    "    lang = filename.name[:2]\n",
    "    #print(lang)\n",
    "\n",
    "    df = pd.read_table(filename, sep='<>', header=None)\n",
    "    df.columns = ['Text']\n",
    "    df['Language'] = lang\n",
    "    #display(df.head())\n",
    "\n",
    "    subset = df.sample(n=300)\n",
    "\n",
    "    out_path = path +r'\\\\' +lang +'.csv'\n",
    "    #print(out_path)\n",
    "    subset.to_csv(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#begin here when re-running\n",
    "folder = r'corpus_folder\\csv'\n",
    "file_names = os.listdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm combining random samples from each language,\n",
    "then removing unnecessary columns, shuffling the new dataframe, and setting a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat(\n",
    "    map(pd.read_csv, ('./corpus_folder/csv/'+i for i in file_names)), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac=1)\n",
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>în scris. - după cum ne-am temut, votul strâns...</td>\n",
       "      <td>ro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>certes, la crise économique générale aggrave l...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oczywiście jestem całkowicie przekonany, że ws...</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we therefore have some great challenges here, ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o parlamento também assumiu uma posição favorá...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Language\n",
       "0  în scris. - după cum ne-am temut, votul strâns...       ro\n",
       "1  certes, la crise économique générale aggrave l...       fr\n",
       "2  oczywiście jestem całkowicie przekonany, że ws...       pl\n",
       "3  we therefore have some great challenges here, ...       en\n",
       "4  o parlamento também assumiu uma posição favorá...       pt"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll prepare the dataframe for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've decided to use NLTK, because I wasn't sure how to use Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df1[\"Text\"]\n",
    "y = df1[\"Language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "# Converting the names of languages to a numerical form\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Text Preprocessing\n",
    "# creating a list for appending the preprocessed text\n",
    "data_list = []\n",
    "# iterating through all the text\n",
    "for text in x:\n",
    "       # removing the symbols and numbers\n",
    "        text = re.sub(r'[!@#$(),n\"%^*?:;~`0-9]', ' ', text)\n",
    "        text = re.sub(r'[[]]', ' ', text)\n",
    "        # converting the text to lower case\n",
    "        text = text.lower()\n",
    "        # appending to data_list\n",
    "        data_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "# Turning the text into a numerical form\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(data_list).toarray()\n",
    "\n",
    "\n",
    "#This uses a huge amount of data --> 1.49 TiB for samples of 15,000\n",
    "# 7,000 -> 502 GB\n",
    "# 1,000 -> 26.5 GB\n",
    "# 700 -> 15.2 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model on a multinomial classification model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"tts\"\n",
    "# Parent Directory path \n",
    "parent_dir = r\".\\corpus_folder\"\n",
    "    \n",
    "# Path \n",
    "path = os.path.join(parent_dir, directory) \n",
    "    \n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the train-test split for model recreation if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these values to a file, because if I run the sampler again,\n",
    "# the data will be shuffled randomly\n",
    "\n",
    "\n",
    "savetxt('./corpus_folder/tts/x_train.txt', x_train, delimiter=',', fmt='%s', encoding='utf8')\n",
    "savetxt('./corpus_folder/tts/y_train.txt', y_train, delimiter=',', fmt='%s', encoding='utf8')\n",
    "savetxt('./corpus_folder/tts/x_test.txt', x_test, delimiter=',', fmt='%s', encoding='utf8')\n",
    "savetxt('./corpus_folder/tts/y_test.txt', y_test, delimiter=',', fmt='%s', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can read the data back in\n",
    "# from numpy import loadtxt\n",
    "# load array\n",
    "# data = loadtxt('./corpus_folder/tts/x_train.txt', delimiter=',', encoding='utf8', fmt='%s')\n",
    "# data = loadtxt('./corpus_folder/tts/y_train.txt', delimiter=',', encoding='utf8', fmt='%s')\n",
    "# data = loadtxt('./corpus_folder/tts/x_test.txt', delimiter=',', encoding='utf8', fmt='%s')\n",
    "# data = loadtxt('./corpus_folder/tts/y_test.txt', delimiter=',', encoding='utf8', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the model accuracy, recall, precision, and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "rc = recall_score(y_test, y_pred, average='micro')\n",
    "pr = precision_score(y_test, y_pred, average='micro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 0.9976190476190476\n",
      "[[67  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 64  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 57  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 59  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 62  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 51  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 57  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 61  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 65  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 62  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 59  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 58  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0 61  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0 59  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0 55  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 62  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 69  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 44  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 66  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 60  0]\n",
      " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 59]]\n",
      "0.9976190476190476\n",
      "0.9976190476190476\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is :\",ac)\n",
    "print(cm)\n",
    "print(rc)\n",
    "print(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"models\"\n",
    "# Parent Directory path \n",
    "parent_dir = r\".\\corpus_folder\"\n",
    "    \n",
    "# Path \n",
    "path = os.path.join(parent_dir, directory) \n",
    "    \n",
    "# Create the directory\n",
    "os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model into a pickle file\n",
    "pickle.dump(model, open(\".\\corpus_folder\\models\\model1.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model back from the pickle file\n",
    "# model1 = pickle.load(open(\".\\corpus_folder\\models\\model1.p\", \"rb\"))\n",
    "# y_pred = model1.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that takes in user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "     x = cv.transform([text]).toarray() # converting text to bag of words model (Vector)\n",
    "     lang = model.predict(x) # predicting the language\n",
    "     lang = le.inverse_transform(lang) # finding the language corresponding the the predicted value\n",
    "     print(\"The langauge is in\",lang[0]) # printing the language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label | Language\n",
    "|----|----|\n",
    "bg | Bulgarian\n",
    "cs| Czech\n",
    "da| Danish\n",
    "de|German\n",
    "el|Greek\n",
    "en|English\n",
    "es|Spanish\n",
    "et|Estonian\n",
    "fi|Finnish\n",
    "fr|French\n",
    "hu|Hungarian\n",
    "it|Italian\n",
    "lt|Lithuanian\n",
    "lv|Latvian\n",
    "nl|Dutch\n",
    "pl|Polish\n",
    "pt|Portuguese\n",
    "ro|Romanian\n",
    "sk|Slovak\n",
    "sl|Slovene\n",
    "sv|Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The langauge is in en\n"
     ]
    }
   ],
   "source": [
    "predict('Hello, my name is John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The langauge is in fr\n"
     ]
    }
   ],
   "source": [
    "predict('Salut, je m\\'appelle John')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The langauge is in es\n"
     ]
    }
   ],
   "source": [
    "predict('Holà me llamo John. Tengo veinte años')\n",
    "\n",
    "# This example was initially incorrect, and required a second sentence before being accurately recognized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The main limitation of this project was a lack of memory on my device. A workaround would be to run this code on a cloud virtual machine, in order to improve the compute ability.\n",
    "* The dataset is based off of speech in a political context. This means the model is limited in its application to speech from other contexts.\n",
    "* The accuracy, precision, and recall of the model is remarkably high at >.997 for all three metrics. I am concerned that the model is overfit to the data.\n",
    "  * A potential fix for this would be to increase the size of the samples.\n",
    "* I am concerned that having languages with starkly different alphabets also contributes to the overfitting. For certain languages all the model needs to do is look for a few specific characters to exclude other languages.\n",
    "  * In the future, I would try to expand the language entries beyond the current 21 languages. I would focus on including alternative languages that use the same or similar alphabets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
